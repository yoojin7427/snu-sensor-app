import os
import io
import csv
import zipfile
import shutil
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from collections import defaultdict
from datetime import datetime, date, time, timedelta, timezone

import streamlit as st
import boto3
from botocore.exceptions import ClientError, NoCredentialsError, ProfileNotFound

from avro.datafile import DataFileReader
from avro.io import DatumReader


# =========================
# âœ… ê³ ì • ì„¤ì •
# =========================
BUCKET = "empatica-us-east-1-prod-data"
REGION = "us-east-1"
DEFAULT_PROG = "1"
DEFAULT_SITE = "1"

APP_DIR = os.path.abspath(os.path.dirname(__file__))
# Streamlit Cloud ë“± ì„œë²„ í™˜ê²½ì„ ê³ ë ¤í•´ ì„ì‹œ í´ë” ê²½ë¡œë¥¼ ì•ˆì „í•˜ê²Œ ì„¤ì •
CACHE_DIR = os.path.join(APP_DIR, "_cache_avro")
OUT_DIR = os.path.join(APP_DIR, "_output_csv")

KST = timezone(timedelta(hours=9))

# âœ… ê¸°ê¸°(ìƒ‰ìƒ) â†” org/profile ë§¤í•‘
DEVICE_MAP = {
    "ğŸ”µ Blue": {"org": "2244", "profile": "deviceA"},
    "ğŸŸ¢ Green": {"org": "2602", "profile": "deviceB"},
}

# Blue ê¸°ë³¸ participant(ìˆìœ¼ë©´ ìë™ ì„ íƒ)
BLUE_DEFAULT_PARTICIPANT = "VRPILOT001-3YKC51P1C9"


# =========================
# ë°ì´í„° í´ë˜ìŠ¤
# =========================
@dataclass
class Window:
    start_kst: datetime
    end_kst: datetime

    @property
    def start_us(self) -> int:
        return int(self.start_kst.astimezone(timezone.utc).timestamp() * 1_000_000)

    @property
    def end_us(self) -> int:
        return int(self.end_kst.astimezone(timezone.utc).timestamp() * 1_000_000)


# =========================
# ìœ í‹¸
# =========================
def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def kst_datetime(d: date, t: time) -> datetime:
    return datetime(d.year, d.month, d.day, t.hour, t.minute, t.second, tzinfo=KST)


def us_to_kst_str(us: int) -> str:
    """epoch microseconds(UTC) -> KST string"""
    dt_utc = datetime.fromtimestamp(us / 1_000_000, tz=timezone.utc)
    return dt_utc.astimezone(KST).strftime("%Y.%m.%d %H:%M:%S")


def compute_timestamp_us(start_us: Optional[int], freq_hz: Optional[float], count: int) -> List[int]:
    """freq_hz == 0/None ë°©ì–´ + ê¹¨ì§„ ê°’ ë°©ì–´"""
    try:
        if start_us is None or count is None or count <= 0:
            return []
        if freq_hz is None:
            return []
        f = float(freq_hz)
        if f <= 0:
            return []
        step = 1e6 / f
        return [int(round(start_us + i * step)) for i in range(int(count))]
    except Exception:
        return []


def clip_rows_by_us(rows: List[List], idx_ts: int, start_us: int, end_us: int) -> List[List]:
    return [r for r in rows if start_us <= int(r[idx_ts]) < end_us]


# =========================
# S3 ê²½ë¡œ/íƒìƒ‰/ë‹¤ìš´ë¡œë“œ
# =========================
def s3_prefix(org: str, prog: str, site: str, date_str: str, participant: str) -> str:
    return f"v2/{org}/{prog}/{site}/participant_data/{date_str}/{participant}/raw_data/v6/"


def participant_root_prefix(org: str, prog: str, site: str, date_str: str) -> str:
    return f"v2/{org}/{prog}/{site}/participant_data/{date_str}/"


def list_participant_folders(s3, org: str, prog: str, site: str, date_str: str) -> List[str]:
    """
    participant_data/{DATE}/ ì•„ë˜ì˜ í´ë”(=participant) ëª©ë¡ì„ ê°€ì ¸ì˜´
    list_objects_v2 + Delimiter="/" ë°©ì‹
    """
    prefix = participant_root_prefix(org, prog, site, date_str)
    folders = []

    paginator = s3.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix, Delimiter="/"):
        for cp in page.get("CommonPrefixes", []):
            p = cp.get("Prefix", "")
            part = p.rstrip("/").split("/")[-1]
            if part:
                folders.append(part)

    return sorted(set(folders))


def list_avro_keys(s3, prefix: str) -> List[str]:
    keys = []
    paginator = s3.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
        for obj in page.get("Contents", []):
            k = obj.get("Key", "")
            if k.lower().endswith(".avro"):
                keys.append(k)
    return keys


def download_avro_files(s3, prefix: str, local_dir: str) -> List[str]:
    ensure_dir(local_dir)
    keys = list_avro_keys(s3, prefix)
    if not keys:
        return []

    local_paths = []
    for key in keys:
        fname = os.path.basename(key)
        dst = os.path.join(local_dir, fname)

        # ìºì‹œ: ì´ë¯¸ ìˆìœ¼ë©´ ì¬ë‹¤ìš´ë¡œë“œ ìƒëµ
        if not os.path.exists(dst) or os.path.getsize(dst) == 0:
            s3.download_file(BUCKET, key, dst)
        local_paths.append(dst)

    return sorted(local_paths)


# =========================
# AVRO â†’ ì„¼ì„œ ì ì¬
# =========================
def load_sensor_data_from_avros(avro_files: List[str]) -> Tuple[Dict[str, List[List]], List[Tuple[str, str, str]]]:
    sensor_data = defaultdict(list)
    skipped_notes = []  # (file, sensor, reason)

    for avro_file in avro_files:
        try:
            reader = DataFileReader(open(avro_file, "rb"), DatumReader())
            data = next(reader)
            reader.close()
        except Exception as e:
            skipped_notes.append((os.path.basename(avro_file), "FILE", f"read_fail: {e}"))
            continue

        raw = (data or {}).get("rawData", {})

        # Accelerometer (digital->physical)
        if "accelerometer" in raw:
            try:
                acc = raw["accelerometer"]
                xs = acc.get("x", [])
                ys = acc.get("y", [])
                zs = acc.get("z", [])
                ts = compute_timestamp_us(acc.get("timestampStart"), acc.get("samplingFrequency"), len(xs))
                if not ts:
                    skipped_notes.append((os.path.basename(avro_file), "accelerometer", "freq<=0 or empty"))
                else:
                    p = acc.get("imuParams", {})
                    dp = (p.get("physicalMax", 0) - p.get("physicalMin", 0))
                    dd = (p.get("digitalMax", 0) - p.get("digitalMin", 0))
                    if dd == 0:
                        skipped_notes.append((os.path.basename(avro_file), "accelerometer", "digital_range=0"))
                    else:
                        x_g = [(v * dp / dd) for v in xs]
                        y_g = [(v * dp / dd) for v in ys]
                        z_g = [(v * dp / dd) for v in zs]
                        sensor_data["accelerometer"].extend([[t, x, y, z] for t, x, y, z in zip(ts, x_g, y_g, z_g)])
            except Exception as e:
                skipped_notes.append((os.path.basename(avro_file), "accelerometer", f"parse_fail: {e}"))

        # Gyroscope
        if "gyroscope" in raw:
            try:
                gyro = raw["gyroscope"]
                xs = gyro.get("x", [])
                ys = gyro.get("y", [])
                zs = gyro.get("z", [])
                ts = compute_timestamp_us(gyro.get("timestampStart"), gyro.get("samplingFrequency"), len(xs))
                if not ts:
                    skipped_notes.append((os.path.basename(avro_file), "gyroscope", "freq<=0 or empty"))
                else:
                    sensor_data["gyroscope"].extend([[t, x, y, z] for t, x, y, z in zip(ts, xs, ys, zs)])
            except Exception as e:
                skipped_notes.append((os.path.basename(avro_file), "gyroscope", f"parse_fail: {e}"))

        # EDA
        if "eda" in raw:
            try:
                eda = raw["eda"]
                vals = eda.get("values", [])
                ts = compute_timestamp_us(eda.get("timestampStart"), eda.get("samplingFrequency"), len(vals))
                if not ts:
                    skipped_notes.append((os.path.basename(avro_file), "eda", "freq<=0 or empty"))
                else:
                    sensor_data["eda"].extend([[t, v] for t, v in zip(ts, vals)])
            except Exception as e:
                skipped_notes.append((os.path.basename(avro_file), "eda", f"parse_fail: {e}"))

        # Temperature
        if "temperature" in raw:
            try:
                tmp = raw["temperature"]
                vals = tmp.get("values", [])
                ts = compute_timestamp_us(tmp.get("timestampStart"), tmp.get("samplingFrequency"), len(vals))
                if not ts:
                    skipped_notes.append((os.path.basename(avro_file), "temperature", "freq<=0 or empty"))
                else:
                    sensor_data["temperature"].extend([[t, v] for t, v in zip(ts, vals)])
            except Exception as e:
                skipped_notes.append((os.path.basename(avro_file), "temperature", f"parse_fail: {e}"))

        # BVP
        if "bvp" in raw:
            try:
                bvp = raw["bvp"]
                vals = bvp.get("values", [])
                ts = compute_timestamp_us(bvp.get("timestampStart"), bvp.get("samplingFrequency"), len(vals))
                if not ts:
                    skipped_notes.append((os.path.basename(avro_file), "bvp", "freq<=0 or empty"))
                else:
                    sensor_data["bvp"].extend([[t, v] for t, v in zip(ts, vals)])
            except Exception as e:
                skipped_notes.append((os.path.basename(avro_file), "bvp", f"parse_fail: {e}"))

        # Steps
        if "steps" in raw:
            try:
                steps = raw["steps"]
                vals = steps.get("values", [])
                ts = compute_timestamp_us(steps.get("timestampStart"), steps.get("samplingFrequency"), len(vals))
                if not ts:
                    skipped_notes.append((os.path.basename(avro_file), "steps", "freq<=0 or empty"))
                else:
                    sensor_data["steps"].extend([[t, v] for t, v in zip(ts, vals)])
            except Exception as e:
                skipped_notes.append((os.path.basename(avro_file), "steps", f"parse_fail: {e}"))

        # Tags (micros)
        if "tags" in raw:
            try:
                tags = raw["tags"]
                for tag_us in tags.get("tagsTimeMicros", []):
                    sensor_data["tags"].append([int(tag_us)])
            except Exception as e:
                skipped_notes.append((os.path.basename(avro_file), "tags", f"parse_fail: {e}"))

        # Systolic Peaks (nanos -> micros)
        if "systolicPeaks" in raw:
            try:
                sps = raw["systolicPeaks"]
                for ns in sps.get("peaksTimeNanos", []):
                    sensor_data["systolic_peaks"].append([int(ns) // 1000])
            except Exception as e:
                skipped_notes.append((os.path.basename(avro_file), "systolic_peaks", f"parse_fail: {e}"))

    for k in sensor_data.keys():
        sensor_data[k].sort(key=lambda r: r[0])

    return sensor_data, skipped_notes


# =========================
# CSV ì €ì¥(time_kst ì»¬ëŸ¼ ì¶”ê°€)
# =========================
def save_csv(path: str, header: List[str], rows: List[List]) -> None:
    ensure_dir(os.path.dirname(path))
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(header)
        w.writerows(rows)


def export_window_to_csv(
    out_root: str,
    participant: str,
    date_str: str,
    window: Window,
    sensor_data: Dict[str, List[List]],
) -> str:
    out_dir = os.path.join(out_root, participant, date_str)
    if os.path.exists(out_dir):
        shutil.rmtree(out_dir)
    ensure_dir(out_dir)

    s_us, e_us = window.start_us, window.end_us

    def with_time_kst(rows_raw: List[List], ts_index: int = 0) -> List[List]:
        out = []
        for r in rows_raw:
            ts = int(r[ts_index])
            out.append([us_to_kst_str(ts), ts] + [c for i, c in enumerate(r) if i != ts_index])
        return out

    if "accelerometer" in sensor_data:
        rows = clip_rows_by_us(sensor_data["accelerometer"], 0, s_us, e_us)
        if rows:
            save_csv(os.path.join(out_dir, "accelerometer.csv"),
                     ["time_kst", "ts_us", "x", "y", "z"], with_time_kst(rows, 0))

    if "gyroscope" in sensor_data:
        rows = clip_rows_by_us(sensor_data["gyroscope"], 0, s_us, e_us)
        if rows:
            save_csv(os.path.join(out_dir, "gyroscope.csv"),
                     ["time_kst", "ts_us", "x", "y", "z"], with_time_kst(rows, 0))

    if "eda" in sensor_data:
        rows = clip_rows_by_us(sensor_data["eda"], 0, s_us, e_us)
        if rows:
            save_csv(os.path.join(out_dir, "eda.csv"),
                     ["time_kst", "ts_us", "eda"], with_time_kst(rows, 0))

    if "temperature" in sensor_data:
        rows = clip_rows_by_us(sensor_data["temperature"], 0, s_us, e_us)
        if rows:
            save_csv(os.path.join(out_dir, "temperature.csv"),
                     ["time_kst", "ts_us", "temperature"], with_time_kst(rows, 0))

    if "bvp" in sensor_data:
        rows = clip_rows_by_us(sensor_data["bvp"], 0, s_us, e_us)
        if rows:
            save_csv(os.path.join(out_dir, "bvp.csv"),
                     ["time_kst", "ts_us", "bvp"], with_time_kst(rows, 0))

    if "steps" in sensor_data:
        rows = clip_rows_by_us(sensor_data["steps"], 0, s_us, e_us)
        if rows:
            save_csv(os.path.join(out_dir, "steps.csv"),
                     ["time_kst", "ts_us", "steps"], with_time_kst(rows, 0))

    if "tags" in sensor_data:
        rows = clip_rows_by_us(sensor_data["tags"], 0, s_us, e_us)
        if rows:
            save_csv(os.path.join(out_dir, "tags.csv"),
                     ["time_kst", "ts_us"], with_time_kst(rows, 0))

    if "systolic_peaks" in sensor_data:
        rows = clip_rows_by_us(sensor_data["systolic_peaks"], 0, s_us, e_us)
        if rows:
            save_csv(os.path.join(out_dir, "systolic_peaks.csv"),
                     ["time_kst", "ts_us"], with_time_kst(rows, 0))

    return out_dir


def make_zip_from_dir(root_dir: str) -> bytes:
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as z:
        for folder, _, files in os.walk(root_dir):
            for f in files:
                abs_path = os.path.join(folder, f)
                rel_path = os.path.relpath(abs_path, root_dir)
                z.write(abs_path, rel_path)
    buf.seek(0)
    return buf.read()


# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="EmbracePlus Downloader", layout="wide")
st.title("EmbracePlus ë°ì´í„° ë‹¤ìš´ë¡œë“œ + CSV ë³€í™˜ (ë‹¨ì¼ ì‹œê°„ë²”ìœ„, KST í‘œê¸°)")

# ğŸš¨ ìˆ˜ì •ëœ ë¶€ë¶„: ì‚¬ì „ ì¤€ë¹„ ì„¤ëª… ì œê±° í›„ ê²½ê³  ë¬¸êµ¬ ì¶”ê°€
st.warning("âš ï¸ **ì£¼ì˜!** ì¶œë°œ ë° ì¢…ë£Œ ì‹œê°„ì„ ì •í™•í•˜ê²Œ ê¸°ì…í•´ì„œ ë‹¤ìš´ë¡œë“œí•´ ì£¼ì„¸ìš”.")

# --- Device(ìƒ‰ìƒ) ì„ íƒ ---
device_choice = st.selectbox("Device ì„ íƒ", list(DEVICE_MAP.keys()), index=0)
org = DEVICE_MAP[device_choice]["org"]
aws_profile = DEVICE_MAP[device_choice]["profile"]

# --- ORG/PROG/SITE/DATE ---
c1, c2, c3 = st.columns(3)
with c1:
    st.text_input("ORG (ìë™)", value=org, disabled=True)
    prog = st.text_input("PROG", value=DEFAULT_PROG)
with c2:
    site = st.text_input("SITE", value=DEFAULT_SITE)
with c3:
    d = st.date_input("DATE (KST ê¸°ì¤€)", value=date.today())  # âœ… ê¸°ë³¸ì€ ì˜¤ëŠ˜
    date_str = d.strftime("%Y-%m-%d")

# --- S3 client ì¤€ë¹„ ---
s3_client = None
s3_err = None

# í˜„ì¬ ì„ íƒëœ ê¸°ê¸°ì˜ í”„ë¡œí•„ ì´ë¦„ (deviceA ë˜ëŠ” deviceB)
target_secret_name = DEVICE_MAP[device_choice]["profile"]

try:
    # 1. ì„œë²„ì˜ ë¹„ë°€ë…¸íŠ¸(Secrets)ì—ì„œ í‚¤ë¥¼ ê°€ì ¸ì˜´
    if target_secret_name in st.secrets:
        creds = st.secrets[target_secret_name]
        
        # 2. ê°€ì ¸ì˜¨ í‚¤ë¡œ S3 ì—°ê²° (boto3.client ì§ì ‘ ìƒì„±)
        s3_client = boto3.client(
            "s3",
            aws_access_key_id=creds["aws_access_key_id"],
            aws_secret_access_key=creds["aws_secret_access_key"],
            region_name="us-east-1"
        )
    else:
        s3_err = f"Secretsì— '{target_secret_name}' ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

except Exception as e:
    s3_err = f"AWS ì—°ê²° ì‹¤íŒ¨: {e}"

# --- participant ëª©ë¡ ìë™ íƒìƒ‰ ---
participants = []
if s3_client and not s3_err:
    try:
        participants = list_participant_folders(s3_client, org, prog, site, date_str)
    except ClientError as e:
        s3_err = f"S3 ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}"
    except Exception as e:
        s3_err = f"ì°¸ê°€ì ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}"

st.subheader("ì°¸ê°€ì ì„ íƒ (S3 ìë™ íƒìƒ‰)")

if s3_err:
    st.warning(s3_err)

if participants:
    # Blueë©´ ê¸°ë³¸ participantê°€ ëª©ë¡ì— ìˆìœ¼ë©´ ê·¸ê±¸ ê¸°ë³¸ ì„ íƒ
    default_idx = 0
    if device_choice.startswith("ğŸ”µ") and BLUE_DEFAULT_PARTICIPANT in participants:
        default_idx = participants.index(BLUE_DEFAULT_PARTICIPANT)

    participant = st.selectbox(
        "PARTICIPANT (S3 í´ë”ëª…)",
        participants,
        index=default_idx
    )
else:
    # ëª©ë¡ì´ ì—†ê±°ë‚˜ ëª» ë¶ˆëŸ¬ì˜¨ ê²½ìš° ìˆ˜ë™ ì…ë ¥ fallback
    fallback_default = BLUE_DEFAULT_PARTICIPANT if device_choice.startswith("ğŸ”µ") else ""
    participant = st.text_input("PARTICIPANT (ìˆ˜ë™ ì…ë ¥, S3 í´ë”ëª… ê·¸ëŒ€ë¡œ)", value=fallback_default)

# --- ì‹œê°„ ë²”ìœ„ ---
st.subheader("ì‹œê°„ ë²”ìœ„ (KST, í•œêµ­ì‹œê°„)")

# ì²´í¬ë°•ìŠ¤ì™€ ë³µì¡í•œ if/else ë¡œì§ì„ ì œê±°í•˜ê³ , ì‹¬í”Œí•˜ê²Œ ë°°ì¹˜
tc1, tc2 = st.columns(2)

with tc1:
    # time_inputì€ ê¸°ë³¸ì ìœ¼ë¡œ í‚¤ë³´ë“œ ì…ë ¥ê³¼ í´ë¦­ ì„ íƒ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    t_start = st.time_input("ì‹œì‘", value=time(10, 0), key="t_start_picker")

with tc2:
    t_end = st.time_input("ì¢…ë£Œ", value=time(11, 0), key="t_end_picker")

st.caption("â€» ë‚´ë¶€ì ìœ¼ë¡œëŠ” UTC epoch(ë§ˆì´í¬ë¡œì´ˆ)ë¡œ ë³€í™˜í•´ì„œ ì •í™•íˆ ìë¦…ë‹ˆë‹¤. CSVì—ëŠ” time_kst ì»¬ëŸ¼ì„ ì¶”ê°€í•©ë‹ˆë‹¤.")

run = st.button("ğŸš€ ë‹¤ìš´ë¡œë“œ + ë³€í™˜ ì‹¤í–‰", type="primary")

if run:
    if s3_err or not s3_client:
        st.error("S3 ì—°ê²°ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ì–´ìš”. ìœ„ ê²½ê³ ë¥¼ í•´ê²°í•´ ì£¼ì„¸ìš”.")
        st.stop()

    if not participant.strip():
        st.error("PARTICIPANTê°€ ë¹„ì–´ìˆì–´ìš”.")
        st.stop()

    start_dt = kst_datetime(d, t_start)
    end_dt = kst_datetime(d, t_end)
    if end_dt <= start_dt:
        st.error("ì¢…ë£Œ ì‹œê°„ì´ ì‹œì‘ ì‹œê°„ë³´ë‹¤ ë¹ ë¥´ê±°ë‚˜ ê°™ì•„ìš”.")
        st.stop()

    window = Window(start_kst=start_dt, end_kst=end_dt)
    prefix = s3_prefix(org, prog, site, date_str, participant)

    st.write(f"ğŸ¨ ì„ íƒ: **{device_choice}** |  ğŸ” profile: `{aws_profile}`")
    st.write(f"ğŸ“Œ S3 Prefix: `{prefix}`")

    local_cache = os.path.join(CACHE_DIR, device_choice.replace(" ", "_"), participant, date_str)
    ensure_dir(local_cache)

    with st.status("S3ì—ì„œ AVRO ë‹¤ìš´ë¡œë“œ ì¤‘...", expanded=True) as status:
        try:
            avro_files = download_avro_files(s3_client, prefix, local_cache)
            if not avro_files:
                status.update(label="AVRO íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", state="error")
                st.error("í•´ë‹¹ ê²½ë¡œì— .avroê°€ ì—†ì–´ìš”. participant/date ê²½ë¡œê°€ ë§ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
                st.stop()

            st.write(f"âœ… AVRO {len(avro_files)}ê°œ ì¤€ë¹„ë¨ (ìºì‹œ: `{local_cache}`)")
            status.update(label="ë‹¤ìš´ë¡œë“œ ì™„ë£Œ", state="complete")
        except ClientError as e:
            st.error(f"S3 ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            st.stop()

    with st.status("AVRO ì½ëŠ” ì¤‘...", expanded=True) as status:
        sensor_data, skipped = load_sensor_data_from_avros(avro_files)
        status.update(label=f"AVRO ë¡œë“œ ì™„ë£Œ (ì„¼ì„œ {len(sensor_data)}ì¢…)", state="complete")

    with st.status("ì‹œê°„ ë²”ìœ„ë¡œ ìë¥´ê³  CSV ìƒì„± ì¤‘...", expanded=True) as status:
        out_dir = export_window_to_csv(OUT_DIR, participant, date_str, window, sensor_data)
        status.update(label="CSV ìƒì„± ì™„ë£Œ", state="complete")

    st.subheader("ê²°ê³¼ ìš”ì•½")
    st.write(f"ğŸ“‚ ì €ì¥ ìœ„ì¹˜(ë¡œì»¬): `{out_dir}`")
    st.write(
        f"â±ï¸ ë²”ìœ„(KST): {start_dt.strftime('%Y.%m.%d %H:%M:%S')} ~ {end_dt.strftime('%Y.%m.%d %H:%M:%S')}"
        f"  (epoch_us {window.start_us} ~ {window.end_us})"
    )

    if skipped:
        with st.expander(f"âš ï¸ ì¼ë¶€ ì„¼ì„œ/íŒŒì¼ ìŠ¤í‚µë¨ ({len(skipped)}ê±´) â€” ê·¸ë˜ë„ ì „ì²´ ë³€í™˜ì€ ì™„ë£Œ", expanded=False):
            st.caption("samplingFrequency=0, ë””ì§€í„¸ ë²”ìœ„=0, íŒŒì¼ ì†ìƒ ë“±ìœ¼ë¡œ ì¼ë¶€ ì„¼ì„œê°€ ìŠ¤í‚µë  ìˆ˜ ìˆì–´ìš”.")
            st.dataframe(
                [{"file": a, "sensor": b, "reason": c} for a, b, c in skipped],
                use_container_width=True
            )

    zip_bytes = make_zip_from_dir(out_dir)
    zip_name = f"{participant}_{date_str}_{t_start.strftime('%H%M')}-{t_end.strftime('%H%M')}_KST_csv.zip"
    st.download_button(
        label="â¬‡ï¸ CSV ZIP ë‹¤ìš´ë¡œë“œ",
        data=zip_bytes,
        file_name=zip_name,
        mime="application/zip",
    )

st.divider()
with st.expander("ğŸ§¯ ìì£¼ ë‚˜ëŠ” ì—ëŸ¬ / í•´ê²°"):
    st.markdown(
        f"""
- **profile ì—†ìŒ**: `aws configure --profile deviceA` ë˜ëŠ” `aws configure --profile deviceB`  
- **ì°¸ê°€ì ëª©ë¡ì´ ë¹„ì–´ ìˆìŒ**: (1) DATEê°€ í‹€ë¦¼ (2) PROG/SITEê°€ ë‹¤ë¦„ (3) í•´ë‹¹ ë‚ ì§œì— ì—…ë¡œë“œ ì•„ì§ ì•ˆ ë¨  
- **AVRO íŒŒì¼ 0ê°œ**: participant/date ê²½ë¡œê°€ ì‹¤ì œ S3 í´ë”ëª…ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨  
- **time_kstê°€ ì´ìƒí•¨**: tags.csvë¡œ KST ë³€í™˜ì´ ë§ëŠ”ì§€ í™•ì¸  
        """
    )
